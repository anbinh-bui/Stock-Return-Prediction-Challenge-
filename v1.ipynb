{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>STOCK</th>\n",
       "      <th>INDUSTRY</th>\n",
       "      <th>INDUSTRY_GROUP</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>RET_1</th>\n",
       "      <th>VOLUME_1</th>\n",
       "      <th>RET_2</th>\n",
       "      <th>...</th>\n",
       "      <th>VOLUME_16</th>\n",
       "      <th>RET_17</th>\n",
       "      <th>VOLUME_17</th>\n",
       "      <th>RET_18</th>\n",
       "      <th>VOLUME_18</th>\n",
       "      <th>RET_19</th>\n",
       "      <th>VOLUME_19</th>\n",
       "      <th>RET_20</th>\n",
       "      <th>VOLUME_20</th>\n",
       "      <th>RET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2377</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.005967</td>\n",
       "      <td>0.136699</td>\n",
       "      <td>0.009031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.493354</td>\n",
       "      <td>-0.007660</td>\n",
       "      <td>-0.585497</td>\n",
       "      <td>-0.001063</td>\n",
       "      <td>-0.351363</td>\n",
       "      <td>0.005127</td>\n",
       "      <td>-0.324675</td>\n",
       "      <td>-0.019275</td>\n",
       "      <td>-0.291751</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5198</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>-0.269520</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313575</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>0.071338</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>-0.405243</td>\n",
       "      <td>-0.003276</td>\n",
       "      <td>-0.424336</td>\n",
       "      <td>-0.010489</td>\n",
       "      <td>-0.050591</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8017</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.014405</td>\n",
       "      <td>0.192655</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.367499</td>\n",
       "      <td>-0.005843</td>\n",
       "      <td>-0.405562</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>-0.315935</td>\n",
       "      <td>0.010462</td>\n",
       "      <td>-0.474957</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>-0.260130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20826</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.430916</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023598</td>\n",
       "      <td>0.011266</td>\n",
       "      <td>0.079711</td>\n",
       "      <td>0.019038</td>\n",
       "      <td>-0.230167</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>-0.312123</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>-0.226628</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33843</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.006523</td>\n",
       "      <td>-0.060371</td>\n",
       "      <td>-0.007632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337686</td>\n",
       "      <td>-0.007224</td>\n",
       "      <td>-0.161117</td>\n",
       "      <td>-0.001461</td>\n",
       "      <td>-0.095494</td>\n",
       "      <td>0.012667</td>\n",
       "      <td>0.471895</td>\n",
       "      <td>-0.038752</td>\n",
       "      <td>1.532045</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418590</th>\n",
       "      <td>391556</td>\n",
       "      <td>206</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>-0.075091</td>\n",
       "      <td>-0.001428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076480</td>\n",
       "      <td>0.017026</td>\n",
       "      <td>0.170516</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>-0.106224</td>\n",
       "      <td>-0.034597</td>\n",
       "      <td>0.123750</td>\n",
       "      <td>-0.015676</td>\n",
       "      <td>-0.228186</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418591</th>\n",
       "      <td>394490</td>\n",
       "      <td>208</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.028099</td>\n",
       "      <td>-0.006688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090287</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>-0.050408</td>\n",
       "      <td>0.008736</td>\n",
       "      <td>-0.159294</td>\n",
       "      <td>0.027350</td>\n",
       "      <td>-0.022922</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>-0.080569</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418592</th>\n",
       "      <td>400150</td>\n",
       "      <td>210</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>-0.004633</td>\n",
       "      <td>-0.173518</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695373</td>\n",
       "      <td>-0.015320</td>\n",
       "      <td>-0.149467</td>\n",
       "      <td>-0.035810</td>\n",
       "      <td>-0.262389</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>-0.172450</td>\n",
       "      <td>0.008586</td>\n",
       "      <td>-0.482171</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418593</th>\n",
       "      <td>403129</td>\n",
       "      <td>211</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.172313</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045512</td>\n",
       "      <td>-0.008823</td>\n",
       "      <td>-0.026153</td>\n",
       "      <td>-0.011428</td>\n",
       "      <td>-0.142636</td>\n",
       "      <td>0.011253</td>\n",
       "      <td>-0.224195</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>-0.341878</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418594</th>\n",
       "      <td>416228</td>\n",
       "      <td>221</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>-0.021391</td>\n",
       "      <td>-0.176100</td>\n",
       "      <td>0.024657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.565365</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>-0.227997</td>\n",
       "      <td>0.032402</td>\n",
       "      <td>-1.587177</td>\n",
       "      <td>-0.043803</td>\n",
       "      <td>-0.387343</td>\n",
       "      <td>-0.006897</td>\n",
       "      <td>-0.467651</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418595 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  DATE  STOCK  INDUSTRY  INDUSTRY_GROUP  SECTOR  SUB_INDUSTRY  \\\n",
       "0         2377     1      0        37              12       5            94   \n",
       "1         5198     4      0        37              12       5            94   \n",
       "2         8017     5      0        37              12       5            94   \n",
       "3        20826    11      0        37              12       5            94   \n",
       "4        33843    21      0        37              12       5            94   \n",
       "...        ...   ...    ...       ...             ...     ...           ...   \n",
       "418590  391556   206   5716        50              17       7           114   \n",
       "418591  394490   208   5716        50              17       7           114   \n",
       "418592  400150   210   5716        50              17       7           114   \n",
       "418593  403129   211   5716        50              17       7           114   \n",
       "418594  416228   221   5716        50              17       7           114   \n",
       "\n",
       "           RET_1  VOLUME_1     RET_2  ...  VOLUME_16    RET_17  VOLUME_17  \\\n",
       "0      -0.005967  0.136699  0.009031  ...  -0.493354 -0.007660  -0.585497   \n",
       "1       0.001348 -0.269520  0.011100  ...  -0.313575  0.007867   0.071338   \n",
       "2      -0.014405  0.192655  0.003614  ...  -0.367499 -0.005843  -0.405562   \n",
       "3       0.008938  0.430916  0.002662  ...   0.023598  0.011266   0.079711   \n",
       "4      -0.006523 -0.060371 -0.007632  ...  -0.337686 -0.007224  -0.161117   \n",
       "...          ...       ...       ...  ...        ...       ...        ...   \n",
       "418590  0.029552 -0.075091 -0.001428  ...   0.076480  0.017026   0.170516   \n",
       "418591  0.008316  0.028099 -0.006688  ...   0.090287  0.002887  -0.050408   \n",
       "418592 -0.004633 -0.173518  0.001687  ...   0.695373 -0.015320  -0.149467   \n",
       "418593  0.010883  0.172313  0.008844  ...  -0.045512 -0.008823  -0.026153   \n",
       "418594 -0.021391 -0.176100  0.024657  ...  -0.565365 -0.002706  -0.227997   \n",
       "\n",
       "          RET_18  VOLUME_18    RET_19  VOLUME_19    RET_20  VOLUME_20    RET  \n",
       "0      -0.001063  -0.351363  0.005127  -0.324675 -0.019275  -0.291751  False  \n",
       "1       0.007733  -0.405243 -0.003276  -0.424336 -0.010489  -0.050591  False  \n",
       "2       0.002930  -0.315935  0.010462  -0.474957 -0.003541  -0.260130   True  \n",
       "3       0.019038  -0.230167 -0.000287  -0.312123  0.008682  -0.226628   True  \n",
       "4      -0.001461  -0.095494  0.012667   0.471895 -0.038752   1.532045  False  \n",
       "...          ...        ...       ...        ...       ...        ...    ...  \n",
       "418590  0.002276  -0.106224 -0.034597   0.123750 -0.015676  -0.228186   True  \n",
       "418591  0.008736  -0.159294  0.027350  -0.022922  0.008186  -0.080569   True  \n",
       "418592 -0.035810  -0.262389  0.000896  -0.172450  0.008586  -0.482171  False  \n",
       "418593 -0.011428  -0.142636  0.011253  -0.224195  0.000609  -0.341878   True  \n",
       "418594  0.032402  -1.587177 -0.043803  -0.387343 -0.006897  -0.467651   True  \n",
       "\n",
       "[418595 rows x 48 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.read_csv('x_train.csv', index_col='ID')\n",
    "y_train = pd.read_csv('y_train.csv', index_col='ID')\n",
    "train = pd.concat([x_train, y_train], axis=1)\n",
    "test = pd.read_csv('x_test.csv', index_col='ID')\n",
    "# Ensure proper time ordering within each stock\n",
    "train = train.sort_values(['STOCK', 'DATE']).reset_index()\n",
    "test  = test.sort_values(['STOCK', 'DATE']).reset_index()\n",
    "train # already sorted by stock and date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Reusable feature engineering pipeline for stock prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-6):\n",
    "        self.eps = eps\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit any parameters needed (e.g., for normalization)\n",
    "        For this dataset, we don't need to fit anything since\n",
    "        features are computed per-stock or per-date\n",
    "        \"\"\"\n",
    "        # No fitting needed for these features\n",
    "        # But this method allows future extensions\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Apply feature engineering to dataframe\n",
    "        \"\"\"\n",
    "        df = df.copy()  # Don't modify original\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"FEATURE ENGINEERING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # =========================================================\n",
    "        # STEP 1: TIME-SERIES FEATURES\n",
    "        # =========================================================\n",
    "        print(\"\\n[Step 1] Creating time-series features...\")\n",
    "        \n",
    "        grouped = df.groupby('STOCK', group_keys=False)\n",
    "        \n",
    "        # Lag features\n",
    "        for lag in [1, 2, 3, 5, 10, 20]:\n",
    "            df[f'RET_1_lag_{lag}'] = grouped['RET_1'].shift(lag)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        for window in [5, 10, 20, 60]:\n",
    "            df[f'RET_1_roll_std_{window}'] = grouped['RET_1'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=max(1, window//2)).std()\n",
    "            )\n",
    "            df[f'RET_1_roll_mean_{window}'] = grouped['RET_1'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=max(1, window//2)).mean()\n",
    "            )\n",
    "        \n",
    "        # Derived features\n",
    "        df['RET_1_vol_adj'] = df['RET_1'] / (df['RET_1_roll_std_20'] + self.eps)\n",
    "        df['RET_1_dist_mean_20'] = df['RET_1'] - df['RET_1_roll_mean_20']\n",
    "                \n",
    "        # =========================================================\n",
    "        # STEP 2: TECHNICAL INDICATORS\n",
    "        # =========================================================\n",
    "        print(\"\\n[Step 2] Creating technical indicators...\")\n",
    "        \n",
    "        # RSI\n",
    "        for period in [14, 28]:\n",
    "            df[f'RSI_{period}'] = grouped['RET_1'].transform(\n",
    "                lambda x: self._calculate_rsi(x, period)\n",
    "            )\n",
    "        \n",
    "        # ADL\n",
    "        df['_mfm'] = np.sign(df['RET_1'])\n",
    "        df['_mfv'] = df['_mfm'] * df['VOLUME_1'].fillna(0)\n",
    "        df['ADL'] = grouped['_mfv'].cumsum()\n",
    "        df['ADL_zscore'] = grouped['ADL'].transform(\n",
    "            lambda x: (x - x.mean()) / (x.std() + self.eps)\n",
    "        )\n",
    "        df['ADL_momentum'] = grouped['ADL'].diff(5)\n",
    "        df.drop(['_mfm', '_mfv'], axis=1, inplace=True)\n",
    "                \n",
    "        # =========================================================\n",
    "        # STEP 3: TURNOVER FEATURES\n",
    "        # =========================================================\n",
    "        print(\"\\n[Step 3] Creating turnover features...\")\n",
    "        \n",
    "        volume_cols = [f'VOLUME_{i}' for i in range(1, 21)]\n",
    "        volume_data = df[volume_cols].values\n",
    "        \n",
    "        df['VOLUME_1_log'] = np.sign(df['VOLUME_1']) * np.log1p(np.abs(df['VOLUME_1']))\n",
    "        \n",
    "        df['turnover_mean_5'] = np.nanmean(volume_data[:, :5], axis=1)\n",
    "        df['turnover_mean_20'] = np.nanmean(volume_data, axis=1)\n",
    "        df['turnover_std_5'] = np.nanstd(volume_data[:, :5], axis=1)\n",
    "        df['turnover_std_20'] = np.nanstd(volume_data, axis=1)\n",
    "        df['turnover_cv_5'] = df['turnover_std_5'] / (np.abs(df['turnover_mean_5']) + self.eps)\n",
    "        df['turnover_cv_20'] = df['turnover_std_20'] / (np.abs(df['turnover_mean_20']) + self.eps)\n",
    "        df['turnover_ratio'] = df['VOLUME_1'] / (df['turnover_mean_20'] + self.eps)\n",
    "        \n",
    "        df['volume_momentum'] = grouped['VOLUME_1'].transform(\n",
    "            lambda x: x.rolling(5, min_periods=1).mean() - x.rolling(20, min_periods=1).mean()\n",
    "        )\n",
    "        df['volume_acceleration'] = grouped['volume_momentum'].diff(1)\n",
    "        df['volume_ret_interaction'] = df['VOLUME_1_log'] * df['RET_1']\n",
    "                \n",
    "        # =========================================================\n",
    "        # STEP 4: LIQUIDITY FEATURES\n",
    "        # =========================================================\n",
    "        print(\"\\n[Step 4] Creating liquidity features...\")\n",
    "        \n",
    "        df['has_volume_data'] = (~df[volume_cols].isna().all(axis=1)).astype(np.int8)\n",
    "        df['volume_nonmissing_frac'] = (1 - df[volume_cols].isna().mean(axis=1))\n",
    "        df['volume_nonmissing_frac'] = grouped['volume_nonmissing_frac'].transform('mean')\n",
    "        df['low_liquidity_flag'] = (df['volume_nonmissing_frac'] < 0.5).astype(np.int8)\n",
    "        df['RET_1_lowliq_interaction'] = df['RET_1'] * df['low_liquidity_flag']\n",
    "                \n",
    "        # =========================================================\n",
    "        # STEP 5: VOLATILITY REGIME FEATURES\n",
    "        # =========================================================\n",
    "        print(\"\\n[Step 5] Creating volatility regime features...\")\n",
    "        \n",
    "        df['vol_regime'] = df.groupby('DATE')['RET_1_roll_std_20'].transform(\n",
    "            lambda x: pd.qcut(x, q=3, labels=False, duplicates='drop') if x.nunique() > 2 else 1\n",
    "        )\n",
    "        df['vol_regime'] = df['vol_regime'].fillna(1).astype(int)\n",
    "        df['high_vol_regime'] = (df['vol_regime'] == 2).astype(np.int8)\n",
    "        df['low_vol_regime'] = (df['vol_regime'] == 0).astype(np.int8)\n",
    "        df['RET_1_regime_adj'] = df['RET_1'] * (1 + df['vol_regime'] * 0.1)\n",
    "                \n",
    "        # =========================================================\n",
    "        # STEP 6: ADVANCED MOMENTUM FEATURES\n",
    "        # =========================================================\n",
    "        print(\"\\n[Step 6] Creating advanced momentum features...\")\n",
    "        \n",
    "        df['reversal_magnitude'] = np.abs(df['RET_1'] - df['RET_1_roll_mean_20'])\n",
    "        df['trend_consistency'] = grouped['RET_1'].transform(\n",
    "            lambda x: (x.rolling(10, min_periods=1).mean() > 0).rolling(10, min_periods=1).mean()\n",
    "        )\n",
    "        df['momentum_strength'] = df['RET_1_roll_mean_20'] / (df['RET_1_roll_std_20'] + self.eps)\n",
    "                \n",
    "        # =========================================================\n",
    "        # STEP 7: CROSS-SECTIONAL FEATURES\n",
    "        # =========================================================\n",
    "        print(\"\\n[Step 7] Creating cross-sectional features...\")\n",
    "        \n",
    "        date_grouped = df.groupby('DATE')\n",
    "        \n",
    "        # Percentile ranks\n",
    "        for col in ['RET_1', 'RET_2', 'RET_5', 'RET_10', 'RET_20']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_pctrank'] = date_grouped[col].rank(pct=True)\n",
    "        \n",
    "        for col in ['VOLUME_1', 'VOLUME_5', 'VOLUME_10']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_pctrank'] = date_grouped[col].rank(pct=True)\n",
    "        \n",
    "        # Z-scores\n",
    "        for col in ['RET_1', 'RET_5', 'RET_10', 'RET_20']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_cs_zscore'] = date_grouped[col].transform(\n",
    "                    lambda x: (x - x.mean()) / (x.std() + self.eps)\n",
    "                )\n",
    "        \n",
    "        # Sector-relative\n",
    "        if 'SECTOR' in df.columns:\n",
    "            sector_date_grouped = df.groupby(['DATE', 'SECTOR'])\n",
    "            \n",
    "            for col in ['RET_1', 'RET_5', 'RET_10']:\n",
    "                if col in df.columns:\n",
    "                    df[f'{col}_sector_pctrank'] = sector_date_grouped[col].rank(pct=True)\n",
    "                    sector_med = sector_date_grouped[col].transform('median')\n",
    "                    df[f'{col}_vs_sector_med'] = df[col] - sector_med\n",
    "            \n",
    "            if 'RET_1_roll_mean_20' in df.columns:\n",
    "                df['sector_strength_rank'] = sector_date_grouped['RET_1_roll_mean_20'].rank(pct=True)\n",
    "        \n",
    "        # Composite momentum\n",
    "        ret_1_5 = [f'RET_{i}' for i in range(1, 6) if f'RET_{i}' in df.columns]\n",
    "        ret_6_20 = [f'RET_{i}' for i in range(6, 21) if f'RET_{i}' in df.columns]\n",
    "        \n",
    "        if ret_1_5:\n",
    "            df['momentum_1_5'] = df[ret_1_5].mean(axis=1)\n",
    "            df['momentum_1_5_pctrank'] = date_grouped['momentum_1_5'].rank(pct=True)\n",
    "        \n",
    "        if ret_6_20:\n",
    "            df['momentum_6_20'] = df[ret_6_20].mean(axis=1)\n",
    "            df['momentum_6_20_pctrank'] = date_grouped['momentum_6_20'].rank(pct=True)\n",
    "        \n",
    "        if ret_1_5:\n",
    "            df['vol_5d'] = df[ret_1_5].std(axis=1)\n",
    "            df['vol_5d_pctrank'] = date_grouped['vol_5d'].rank(pct=True)\n",
    "            df['ret_vol_ratio'] = df.get('momentum_1_5', 0) / (df['vol_5d'] + self.eps)\n",
    "            df['ret_vol_ratio_pctrank'] = date_grouped['ret_vol_ratio'].rank(pct=True)\n",
    "                \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"✅ FEATURE ENGINEERING COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def _calculate_rsi(self, series, period=14):\n",
    "        \"\"\"Helper: Calculate RSI\"\"\"\n",
    "        delta = series.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=period//2).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=period//2).mean()\n",
    "        rs = gain / (loss + self.eps)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "[Step 1] Creating time-series features...\n",
      "\n",
      "[Step 2] Creating technical indicators...\n",
      "\n",
      "[Step 3] Creating turnover features...\n",
      "\n",
      "[Step 4] Creating liquidity features...\n",
      "\n",
      "[Step 5] Creating volatility regime features...\n",
      "\n",
      "[Step 6] Creating advanced momentum features...\n",
      "\n",
      "[Step 7] Creating cross-sectional features...\n",
      "\n",
      "================================================================================\n",
      "✅ FEATURE ENGINEERING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fe = FeatureEngineer(eps=1e-6)\n",
    "\n",
    "train['_dataset'] = 'train'\n",
    "test['_dataset'] = 'test'\n",
    "df_combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "df_engineered = fe.fit_transform(df_combined)\n",
    "\n",
    "train_eng = df_engineered[df_engineered['_dataset'] == 'train'].drop('_dataset', axis=1).reset_index(drop=True)\n",
    "test_eng = df_engineered[df_engineered['_dataset'] == 'test'].drop('_dataset', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>STOCK</th>\n",
       "      <th>INDUSTRY</th>\n",
       "      <th>INDUSTRY_GROUP</th>\n",
       "      <th>SECTOR</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>RET_1</th>\n",
       "      <th>VOLUME_1</th>\n",
       "      <th>RET_2</th>\n",
       "      <th>...</th>\n",
       "      <th>RET_10_vs_sector_med</th>\n",
       "      <th>sector_strength_rank</th>\n",
       "      <th>momentum_1_5</th>\n",
       "      <th>momentum_1_5_pctrank</th>\n",
       "      <th>momentum_6_20</th>\n",
       "      <th>momentum_6_20_pctrank</th>\n",
       "      <th>vol_5d</th>\n",
       "      <th>vol_5d_pctrank</th>\n",
       "      <th>ret_vol_ratio</th>\n",
       "      <th>ret_vol_ratio_pctrank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2377</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.005967</td>\n",
       "      <td>0.136699</td>\n",
       "      <td>0.009031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.366182</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.790854</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.733974</td>\n",
       "      <td>0.654023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5198</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>-0.269520</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.623980</td>\n",
       "      <td>-0.001430</td>\n",
       "      <td>0.145087</td>\n",
       "      <td>0.011573</td>\n",
       "      <td>0.278822</td>\n",
       "      <td>0.235824</td>\n",
       "      <td>0.676481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8017</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.014405</td>\n",
       "      <td>0.192655</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.670762</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.747280</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.074763</td>\n",
       "      <td>0.065499</td>\n",
       "      <td>0.709372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20826</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.430916</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005347</td>\n",
       "      <td>0.761224</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.902721</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.112585</td>\n",
       "      <td>0.803225</td>\n",
       "      <td>0.945918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33843</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>-0.006523</td>\n",
       "      <td>-0.060371</td>\n",
       "      <td>-0.007632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006162</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002009</td>\n",
       "      <td>0.451809</td>\n",
       "      <td>-0.005748</td>\n",
       "      <td>0.187352</td>\n",
       "      <td>0.006528</td>\n",
       "      <td>0.148461</td>\n",
       "      <td>-0.307673</td>\n",
       "      <td>0.354075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418590</th>\n",
       "      <td>391556</td>\n",
       "      <td>206</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>0.029552</td>\n",
       "      <td>-0.075091</td>\n",
       "      <td>-0.001428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>0.045840</td>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.751933</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>0.799016</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.323612</td>\n",
       "      <td>0.693922</td>\n",
       "      <td>0.837316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418591</th>\n",
       "      <td>394490</td>\n",
       "      <td>208</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.028099</td>\n",
       "      <td>-0.006688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010926</td>\n",
       "      <td>0.038285</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.774029</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.570893</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>0.334697</td>\n",
       "      <td>0.517537</td>\n",
       "      <td>0.861963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418592</th>\n",
       "      <td>400150</td>\n",
       "      <td>210</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>-0.004633</td>\n",
       "      <td>-0.173518</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.016897</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>0.830601</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.719166</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.417350</td>\n",
       "      <td>0.463626</td>\n",
       "      <td>0.868511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418593</th>\n",
       "      <td>403129</td>\n",
       "      <td>211</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>0.010883</td>\n",
       "      <td>0.172313</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003665</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>0.767827</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>0.115580</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>0.048327</td>\n",
       "      <td>1.949107</td>\n",
       "      <td>0.984792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418594</th>\n",
       "      <td>416228</td>\n",
       "      <td>221</td>\n",
       "      <td>5716</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>114</td>\n",
       "      <td>-0.021391</td>\n",
       "      <td>-0.176100</td>\n",
       "      <td>0.024657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006337</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>-0.009799</td>\n",
       "      <td>0.361315</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.699196</td>\n",
       "      <td>0.030331</td>\n",
       "      <td>0.689752</td>\n",
       "      <td>-0.323043</td>\n",
       "      <td>0.462399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418595 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  DATE  STOCK  INDUSTRY  INDUSTRY_GROUP  SECTOR  SUB_INDUSTRY  \\\n",
       "0         2377     1      0        37              12       5            94   \n",
       "1         5198     4      0        37              12       5            94   \n",
       "2         8017     5      0        37              12       5            94   \n",
       "3        20826    11      0        37              12       5            94   \n",
       "4        33843    21      0        37              12       5            94   \n",
       "...        ...   ...    ...       ...             ...     ...           ...   \n",
       "418590  391556   206   5716        50              17       7           114   \n",
       "418591  394490   208   5716        50              17       7           114   \n",
       "418592  400150   210   5716        50              17       7           114   \n",
       "418593  403129   211   5716        50              17       7           114   \n",
       "418594  416228   221   5716        50              17       7           114   \n",
       "\n",
       "           RET_1  VOLUME_1     RET_2  ...  RET_10_vs_sector_med  \\\n",
       "0      -0.005967  0.136699  0.009031  ...             -0.007893   \n",
       "1       0.001348 -0.269520  0.011100  ...             -0.002215   \n",
       "2      -0.014405  0.192655  0.003614  ...              0.009023   \n",
       "3       0.008938  0.430916  0.002662  ...              0.011519   \n",
       "4      -0.006523 -0.060371 -0.007632  ...             -0.006162   \n",
       "...          ...       ...       ...  ...                   ...   \n",
       "418590  0.029552 -0.075091 -0.001428  ...              0.009727   \n",
       "418591  0.008316  0.028099 -0.006688  ...             -0.010926   \n",
       "418592 -0.004633 -0.173518  0.001687  ...              0.001495   \n",
       "418593  0.010883  0.172313  0.008844  ...             -0.003665   \n",
       "418594 -0.021391 -0.176100  0.024657  ...              0.006337   \n",
       "\n",
       "        sector_strength_rank  momentum_1_5  momentum_1_5_pctrank  \\\n",
       "0                        NaN      0.005347              0.366182   \n",
       "1                        NaN      0.002730              0.623980   \n",
       "2                        NaN      0.000613              0.670762   \n",
       "3                        NaN      0.005347              0.761224   \n",
       "4                        NaN     -0.002009              0.451809   \n",
       "...                      ...           ...                   ...   \n",
       "418590              0.045840      0.009303              0.751933   \n",
       "418591              0.038285      0.004816              0.774029   \n",
       "418592              0.016897      0.004498              0.830601   \n",
       "418593              0.015748      0.008220              0.767827   \n",
       "418594              0.032258     -0.009799              0.361315   \n",
       "\n",
       "        momentum_6_20  momentum_6_20_pctrank    vol_5d  vol_5d_pctrank  \\\n",
       "0            0.001468               0.790854  0.007284        0.137540   \n",
       "1           -0.001430               0.145087  0.011573        0.278822   \n",
       "2            0.001354               0.747280  0.009360        0.074763   \n",
       "3            0.004605               0.902721  0.006656        0.112585   \n",
       "4           -0.005748               0.187352  0.006528        0.148461   \n",
       "...               ...                    ...       ...             ...   \n",
       "418590       0.007490               0.799016  0.013405        0.323612   \n",
       "418591       0.004637               0.570893  0.009304        0.334697   \n",
       "418592       0.003175               0.719166  0.009700        0.417350   \n",
       "418593      -0.001501               0.115580  0.004216        0.048327   \n",
       "418594       0.001974               0.699196  0.030331        0.689752   \n",
       "\n",
       "        ret_vol_ratio  ret_vol_ratio_pctrank  \n",
       "0            0.733974               0.654023  \n",
       "1            0.235824               0.676481  \n",
       "2            0.065499               0.709372  \n",
       "3            0.803225               0.945918  \n",
       "4           -0.307673               0.354075  \n",
       "...               ...                    ...  \n",
       "418590       0.693922               0.837316  \n",
       "418591       0.517537               0.861963  \n",
       "418592       0.463626               0.868511  \n",
       "418593       1.949107               0.984792  \n",
       "418594      -0.323043               0.462399  \n",
       "\n",
       "[418595 rows x 121 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-modeling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total features: 113\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TOP 30 FEATURES BY CORRELATION WITH TARGET\n",
      "--------------------------------------------------------------------------------\n",
      "volume_nonmissing_frac    0.028041\n",
      "vol_5d_pctrank            0.025208\n",
      "VOLUME_1_pctrank          0.023099\n",
      "RET_1_sector_pctrank      0.022385\n",
      "RET_1_pctrank             0.022052\n",
      "RET_1_roll_std_60         0.021728\n",
      "RET_1_vs_sector_med       0.019945\n",
      "RET_1_cs_zscore           0.019352\n",
      "RET_1_vol_adj             0.019279\n",
      "RET_1_roll_std_20         0.017856\n",
      "RSI_14                    0.016891\n",
      "RET_1                     0.016845\n",
      "vol_regime                0.016397\n",
      "RET_1_regime_adj          0.016211\n",
      "momentum_1_5_pctrank      0.016190\n",
      "VOLUME_1_log              0.015627\n",
      "RET_1_roll_std_10         0.015253\n",
      "RET_1_dist_mean_20        0.014709\n",
      "low_vol_regime            0.014335\n",
      "RET_17                    0.014077\n",
      "high_vol_regime           0.013305\n",
      "RET_16                    0.012448\n",
      "RET_2_pctrank             0.012071\n",
      "RET_10_pctrank            0.011948\n",
      "vol_5d                    0.011810\n",
      "RET_1_roll_std_5          0.011683\n",
      "momentum_1_5              0.011357\n",
      "RET_1_roll_mean_60        0.010475\n",
      "RSI_28                    0.010240\n",
      "VOLUME_5_pctrank          0.010240\n",
      "Name: RET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['DATE', 'STOCK', 'INDUSTRY', 'INDUSTRY_GROUP', 'SECTOR', 'SUB_INDUSTRY']\n",
    "exclude_cols = ['ID', 'RET'] + categorical_cols\n",
    "\n",
    "# Get all numeric features\n",
    "feature_cols = [col for col in train_eng.columns if col not in exclude_cols]\n",
    "print(f\"\\nTotal features: {len(feature_cols)}\")\n",
    "\n",
    "# ----- 1.1: Correlation with target -----\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TOP 30 FEATURES BY CORRELATION WITH TARGET\")\n",
    "print(\"-\"*80)\n",
    "target_corr = train_eng[feature_cols + ['RET']].corr()['RET'].drop('RET').abs().sort_values(ascending=False)\n",
    "print(target_corr.head(30))\n",
    "\n",
    "# Save for later use\n",
    "top_50_by_corr = target_corr.head(50).index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MULTICOLLINEARITY CHECK (Highly correlated feature pairs)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Found 3 highly correlated pairs (r > 0.95):\n",
      "         feature_1          feature_2  correlation\n",
      "             RET_1   RET_1_regime_adj     0.998498\n",
      "             RET_1 RET_1_dist_mean_20     0.963680\n",
      "RET_1_dist_mean_20   RET_1_regime_adj     0.962721\n",
      "\n",
      "⚠️  Consider removing 2 redundant features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"MULTICOLLINEARITY CHECK (Highly correlated feature pairs)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = train_eng[feature_cols].corr().abs()\n",
    "\n",
    "# Find highly correlated pairs (>0.95)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.95:\n",
    "            high_corr_pairs.append({\n",
    "                'feature_1': corr_matrix.columns[i],\n",
    "                'feature_2': corr_matrix.columns[j],\n",
    "                'correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('correlation', ascending=False)\n",
    "    print(f\"\\nFound {len(high_corr_pairs)} highly correlated pairs (r > 0.95):\")\n",
    "    print(high_corr_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # Features to potentially remove (keep one from each pair)\n",
    "    redundant_features = set(high_corr_df['feature_2'].tolist())\n",
    "    print(f\"\\n⚠️  Consider removing {len(redundant_features)} redundant features\")\n",
    "else:\n",
    "    print(\"✓ No severe multicollinearity detected (all correlations < 0.95)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "LOW VARIANCE FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "Found 34 features with very low variance:\n",
      "RET_1_lowliq_interaction    0.000012\n",
      "RET_1_roll_mean_60          0.000027\n",
      "RET_1_roll_mean_20          0.000069\n",
      "RET_1_roll_mean_10          0.000122\n",
      "RET_1_roll_std_60           0.000171\n",
      "momentum_1_5                0.000180\n",
      "momentum_6_20               0.000215\n",
      "RET_1_roll_mean_5           0.000230\n",
      "RET_1_roll_std_20           0.000246\n",
      "RET_1_roll_std_10           0.000308\n",
      "RET_1_roll_std_5            0.000394\n",
      "vol_5d                      0.000474\n",
      "reversal_magnitude          0.000555\n",
      "RET_5_vs_sector_med         0.000746\n",
      "RET_1_vs_sector_med         0.000747\n",
      "RET_10_vs_sector_med        0.000784\n",
      "RET_16                      0.000872\n",
      "RET_1_dist_mean_20          0.000901\n",
      "RET_14                      0.000923\n",
      "RET_11                      0.000944\n",
      "RET_19                      0.000945\n",
      "RET_7                       0.000959\n",
      "RET_2                       0.000960\n",
      "RET_1_lag_20                0.000971\n",
      "RET_4                       0.000975\n",
      "RET_1_lag_2                 0.000978\n",
      "RET_1_lag_10                0.000978\n",
      "RET_1_lag_1                 0.000979\n",
      "RET_1_lag_5                 0.000979\n",
      "RET_1                       0.000980\n",
      "RET_6                       0.000980\n",
      "RET_3                       0.000982\n",
      "RET_1_lag_3                 0.000982\n",
      "RET_18                      0.000988\n",
      "dtype: float64\n",
      "\n",
      "⚠️  Consider removing these features\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"LOW VARIANCE FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "feature_variance = train_eng[feature_cols].var().sort_values()\n",
    "low_var_features = feature_variance[feature_variance < 0.001].index.tolist()\n",
    "\n",
    "if low_var_features:\n",
    "    print(f\"Found {len(low_var_features)} features with very low variance:\")\n",
    "    print(feature_variance[feature_variance < 0.001])\n",
    "    print(\"\\n⚠️  Consider removing these features\")\n",
    "else:\n",
    "    print(\"✓ All features have sufficient variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE CLEANUP\n",
      "================================================================================\n",
      "\n",
      "Original features: 113\n",
      "Removing: 2\n",
      "Final features: 111\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE CLEANUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Remove multicollinear features (definite removes)\n",
    "redundant_features = ['RET_1_regime_adj', 'RET_1_dist_mean_20']\n",
    "\n",
    "# DON'T remove low variance features - they're fine for returns!\n",
    "# low_var_features = []  # Leave this empty\n",
    "\n",
    "# Combine removals\n",
    "features_to_remove = redundant_features\n",
    "features_to_use = [f for f in feature_cols if f not in features_to_remove]\n",
    "\n",
    "print(f\"\\nOriginal features: {len(feature_cols)}\")\n",
    "print(f\"Removing: {len(features_to_remove)}\")\n",
    "print(f\"Final features: {len(features_to_use)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA PREPARATION\n",
      "================================================================================\n",
      "\n",
      "Checking for problematic columns...\n",
      "⚠️  Found 3 all-NaN columns:\n",
      "  ✗ volume_directional_flow\n",
      "  ✗ avg_volume_on_extreme\n",
      "  ✗ turnover_ret_rank_corr\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "categorical_cols = ['DATE', 'STOCK', 'INDUSTRY', 'INDUSTRY_GROUP', 'SECTOR', 'SUB_INDUSTRY']\n",
    "exclude_cols = ['ID', 'RET'] + categorical_cols\n",
    "\n",
    "X_train = train_eng[features_to_use].copy()\n",
    "y_train = train_eng['RET'].astype(int)\n",
    "groups_train = train_eng['DATE'].values\n",
    "\n",
    "X_test = test_eng[features_to_use].copy()\n",
    "\n",
    "print(\"\\nChecking for problematic columns...\")\n",
    "\n",
    "# Check for all-NaN columns\n",
    "all_nan_cols = X_train.columns[X_train.isna().all()].tolist()\n",
    "if all_nan_cols:\n",
    "    print(f\"⚠️  Found {len(all_nan_cols)} all-NaN columns:\")\n",
    "    for col in all_nan_cols:\n",
    "        print(f\"  ✗ {col}\")\n",
    "    X_train = X_train.drop(columns=all_nan_cols)\n",
    "    X_test = X_test.drop(columns=all_nan_cols)\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric_cols = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"⚠️  Found {len(non_numeric_cols)} non-numeric columns:\")\n",
    "    for col in non_numeric_cols:\n",
    "        print(f\"  ✗ {col}\")\n",
    "    X_train = X_train.drop(columns=non_numeric_cols)\n",
    "    X_test = X_test.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Check for columns with >95% NaN\n",
    "high_nan_cols = X_train.columns[X_train.isna().mean() > 0.95].tolist()\n",
    "if high_nan_cols:\n",
    "    print(f\"⚠️  Found {len(high_nan_cols)} columns with >95% NaN:\")\n",
    "    for col in high_nan_cols:\n",
    "        print(f\"  ✗ {col} ({X_train[col].isna().mean()*100:.1f}% NaN)\")\n",
    "    X_train = X_train.drop(columns=high_nan_cols)\n",
    "    X_test = X_test.drop(columns=high_nan_cols)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,  # Now column count matches!\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_imputed),\n",
    "    columns=X_train_imputed.columns,\n",
    "    index=X_train_imputed.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_imputed),\n",
    "    columns=X_test_imputed.columns,\n",
    "    index=X_test_imputed.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Running 5-fold time series cross-validation...\n",
      "\n",
      "Fold 1: Acc=0.5131, AUC=0.5195, LogLoss=0.6929\n",
      "Fold 2: Acc=0.5432, AUC=0.5588, LogLoss=0.6876\n",
      "Fold 3: Acc=0.5351, AUC=0.5483, LogLoss=0.6887\n",
      "Fold 4: Acc=0.5372, AUC=0.5512, LogLoss=0.6884\n",
      "Fold 5: Acc=0.5401, AUC=0.5606, LogLoss=0.6871\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Mean Accuracy: 0.5337 ± 0.0119\n",
      "Mean AUC:      0.5477 ± 0.0166\n",
      "Mean LogLoss:  0.6889 ± 0.0023\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "baseline_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "cv_results = []\n",
    "feature_importance_list = []\n",
    "\n",
    "print(\"\\nRunning 5-fold time series cross-validation...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_scaled), 1):\n",
    "    X_tr, X_val = X_train_scaled.iloc[train_idx], X_train_scaled.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    baseline_model.fit(X_tr, y_tr, verbose=False)\n",
    "    \n",
    "    y_pred = baseline_model.predict(X_val)\n",
    "    y_prob = baseline_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_prob)\n",
    "    logloss = log_loss(y_val, y_prob)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': acc,\n",
    "        'auc': auc,\n",
    "        'logloss': logloss\n",
    "    })\n",
    "    \n",
    "    feature_importance_list.append(baseline_model.feature_importances_)\n",
    "    \n",
    "    print(f\"Fold {fold}: Acc={acc:.4f}, AUC={auc:.4f}, LogLoss={logloss:.4f}\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"Mean Accuracy: {cv_df['accuracy'].mean():.4f} ± {cv_df['accuracy'].std():.4f}\")\n",
    "print(f\"Mean AUC:      {cv_df['auc'].mean():.4f} ± {cv_df['auc'].std():.4f}\")\n",
    "print(f\"Mean LogLoss:  {cv_df['logloss'].mean():.4f} ± {cv_df['logloss'].std():.4f}\")\n",
    "print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "TOP 30 FEATURES BY MODEL IMPORTANCE:\n",
      "               feature  importance  importance_std  target_corr\n",
      "volume_nonmissing_frac    0.016696        0.003750     0.028041\n",
      "       RET_1_cs_zscore    0.013104        0.001275     0.019352\n",
      "                RET_17    0.012707        0.002100     0.014077\n",
      "                vol_5d    0.012294        0.001637     0.011810\n",
      "                 RET_1    0.012036        0.002027     0.016845\n",
      "          VOLUME_1_log    0.011889        0.001202     0.015627\n",
      "        vol_5d_pctrank    0.011460        0.001133     0.025208\n",
      "              VOLUME_1    0.011387        0.000551     0.004757\n",
      "  momentum_1_5_pctrank    0.011328        0.000879     0.016190\n",
      "         momentum_6_20    0.011185        0.000341     0.005491\n",
      "         RET_1_vol_adj    0.011160        0.000298     0.019279\n",
      "   RET_1_vs_sector_med    0.011057        0.001146     0.019945\n",
      "          momentum_1_5    0.010931        0.000283     0.011357\n",
      "      VOLUME_1_pctrank    0.010824        0.000989     0.023099\n",
      "    RET_1_roll_mean_60    0.010806        0.000686     0.010475\n",
      "                 RET_7    0.010763        0.000460     0.005828\n",
      "         ret_vol_ratio    0.010759        0.000689     0.008086\n",
      "            vol_regime    0.010609        0.002788     0.016397\n",
      "                RET_13    0.010573        0.000879     0.005953\n",
      "         RET_2_pctrank    0.010510        0.000563     0.012071\n",
      "                RET_16    0.010494        0.000196     0.012448\n",
      "                 RET_2    0.010426        0.000449     0.009127\n",
      "                 RET_3    0.010418        0.000664     0.003480\n",
      "                 RET_4    0.010275        0.000605     0.005003\n",
      "     RET_1_roll_std_60    0.010139        0.000618     0.021728\n",
      "                RET_14    0.010100        0.000351     0.001362\n",
      "        RET_10_pctrank    0.010078        0.000834     0.011948\n",
      " momentum_6_20_pctrank    0.010032        0.000747     0.005299\n",
      "                RSI_14    0.009997        0.000522     0.016891\n",
      "                RET_15    0.009990        0.000359     0.004018\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE BY FEATURE CATEGORY\n",
      "--------------------------------------------------------------------------------\n",
      "                      sum      mean  count\n",
      "category                                  \n",
      "Turnover/Volume  0.329668  0.009157     36\n",
      "Other            0.217569  0.009890     22\n",
      "Cross-sectional  0.153382  0.009586     16\n",
      "Time-series      0.127989  0.008533     15\n",
      "Volatility       0.064829  0.008104      8\n",
      "Momentum         0.062379  0.010396      6\n",
      "Technical        0.044184  0.008837      5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_importance = np.mean(feature_importance_list, axis=0)\n",
    "target_corr_aligned = target_corr.reindex(X_train_scaled.columns).fillna(0).abs()\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train_scaled.columns,\n",
    "    'importance': avg_importance,\n",
    "    'importance_std': np.std(feature_importance_list, axis=0),\n",
    "    'target_corr': target_corr_aligned.values\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTOP 30 FEATURES BY MODEL IMPORTANCE:\")\n",
    "print(importance_df.head(30).to_string(index=False))\n",
    "\n",
    "# Categorize features\n",
    "def categorize_feature(feat):\n",
    "    if 'RSI' in feat or 'ADL' in feat:\n",
    "        return 'Technical'\n",
    "    elif 'turnover' in feat or 'volume' in feat.lower():\n",
    "        return 'Turnover/Volume'\n",
    "    elif 'regime' in feat or 'vol_' in feat:\n",
    "        return 'Volatility'\n",
    "    elif 'momentum' in feat or 'reversal' in feat:\n",
    "        return 'Momentum'\n",
    "    elif 'pctrank' in feat or 'zscore' in feat or 'sector' in feat:\n",
    "        return 'Cross-sectional'\n",
    "    elif 'lag' in feat or 'roll' in feat:\n",
    "        return 'Time-series'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "importance_df['category'] = importance_df['feature'].apply(categorize_feature)\n",
    "category_importance = importance_df.groupby('category')['importance'].agg(['sum', 'mean', 'count']).sort_values('sum', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"IMPORTANCE BY FEATURE CATEGORY\")\n",
    "print(\"-\"*80)\n",
    "print(category_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING FEATURE SELECTION\n",
      "================================================================================\n",
      "\n",
      "Comparing feature sets:\n",
      "\n",
      "All features (108)................................ 0.5337 (53.37%)\n",
      "Top 50 features................................... 0.5340 (53.40%)\n",
      "Top 30 features................................... 0.5338 (53.38%)\n",
      "\n",
      "================================================================================\n",
      "✅ Top 50 features BETTER by 0.03pp!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TEST REDUCED FEATURE SETS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get top features by importance\n",
    "top_50_features = importance_df.head(50)['feature'].tolist()\n",
    "top_30_features = importance_df.head(30)['feature'].tolist()\n",
    "\n",
    "# Compare performance\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "def quick_eval(features, name):\n",
    "    X_subset = X_train_scaled[features]\n",
    "    cv_acc = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(X_subset):\n",
    "        X_tr, X_val = X_subset.iloc[train_idx], X_subset.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=200, max_depth=5, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.7, random_state=42\n",
    "        )\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "        cv_acc.append(accuracy_score(y_val, model.predict(X_val)))\n",
    "    \n",
    "    mean_acc = np.mean(cv_acc)\n",
    "    print(f\"{name:.<50} {mean_acc:.4f} ({mean_acc*100:.2f}%)\")\n",
    "    return mean_acc\n",
    "\n",
    "print(\"\\nComparing feature sets:\\n\")\n",
    "acc_all = quick_eval(X_train_scaled.columns.tolist(), f\"All features ({len(X_train_scaled.columns)})\")\n",
    "acc_50 = quick_eval(top_50_features, \"Top 50 features\")\n",
    "acc_30 = quick_eval(top_30_features, \"Top 30 features\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "if acc_50 > acc_all:\n",
    "    print(f\"✅ Top 50 features BETTER by {(acc_50-acc_all)*100:.2f}pp!\")\n",
    "elif acc_30 > acc_all:\n",
    "    print(f\"✅ Top 30 features BETTER by {(acc_30-acc_all)*100:.2f}pp!\")\n",
    "else:\n",
    "    print(f\"✅ All features optimal\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "Running random search (this may take 5-10 minutes)...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "================================================================================\n",
      "Best parameters found:\n",
      "================================================================================\n",
      "  subsample..................... 0.9\n",
      "  reg_lambda.................... 1\n",
      "  reg_alpha..................... 0.1\n",
      "  n_estimators.................. 500\n",
      "  min_child_weight.............. 1\n",
      "  max_depth..................... 7\n",
      "  learning_rate................. 0.07\n",
      "  gamma......................... 0\n",
      "  colsample_bytree.............. 0.6\n",
      "\n",
      "Best CV AUC: 0.5654\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define search space\n",
    "param_distributions = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [4, 5, 6, 7],\n",
    "    'learning_rate': [0.03, 0.05, 0.07, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "# Use best feature set (e.g., top 50)\n",
    "X_train_best = X_train_scaled[top_50_features]\n",
    "\n",
    "# Random search with time series CV\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Try 20 random combinations\n",
    "    scoring='roc_auc',\n",
    "    cv=TimeSeriesSplit(n_splits=3),  # Faster with 3 folds\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nRunning random search (this may take 5-10 minutes)...\")\n",
    "random_search.fit(X_train_best, y_train)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Best parameters found:\")\n",
    "print(f\"{'='*80}\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param:.<30} {value}\")\n",
    "\n",
    "print(f\"\\nBest CV AUC: {random_search.best_score_:.4f}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save best model\n",
    "best_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_best = X_test_scaled[top_50_features]\n",
    "\n",
    "# Predict\n",
    "test_preds = best_model.predict(X_test_best)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_eng['ID'],\n",
    "    'RET': test_preds\n",
    "})\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission_tuned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENSEMBLE MODEL\n",
      "================================================================================\n",
      "\n",
      "Training ensemble (XGBoost + LightGBM + CatBoost)...\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING SET PERFORMANCE\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy:  0.6634 (66.34%)\n",
      "AUC:       0.7309\n",
      "LogLoss:   0.6568\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENSEMBLE: XGBoost + LightGBM + CatBoost\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Use best features\n",
    "X_train_best = X_train_scaled[top_50_features]\n",
    "X_test_best = X_test_scaled[top_50_features]\n",
    "\n",
    "# Train 3 models\n",
    "print(\"\\nTraining ensemble (XGBoost + LightGBM + CatBoost)...\")\n",
    "\n",
    "# Model 1: XGBoost\n",
    "xgb_model = xgb.XGBClassifier(**random_search.best_params_, random_state=42)\n",
    "xgb_model.fit(X_train_best, y_train, verbose=False)\n",
    "\n",
    "# Model 2: LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.7, random_state=42, verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train_best, y_train)\n",
    "\n",
    "# Model 3: CatBoost\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=300, depth=6, learning_rate=0.05,\n",
    "    random_state=42, verbose=0\n",
    ")\n",
    "cat_model.fit(X_train_best, y_train)\n",
    "\n",
    "# Predict on training data\n",
    "xgb_train_probs = xgb_model.predict_proba(X_train_best)[:, 1]\n",
    "lgb_train_probs = lgb_model.predict_proba(X_train_best)[:, 1]\n",
    "cat_train_probs = cat_model.predict_proba(X_train_best)[:, 1]\n",
    "\n",
    "# Ensemble on training data\n",
    "ensemble_train_probs = (xgb_train_probs + lgb_train_probs + cat_train_probs) / 3\n",
    "ensemble_train_preds = (ensemble_train_probs > 0.5).astype(int)\n",
    "\n",
    "# Training metrics\n",
    "train_acc = accuracy_score(y_train, ensemble_train_preds)\n",
    "train_auc = roc_auc_score(y_train, ensemble_train_probs)\n",
    "train_logloss = log_loss(y_train, ensemble_train_probs)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"TRAINING SET PERFORMANCE\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Accuracy:  {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"AUC:       {train_auc:.4f}\")\n",
    "print(f\"LogLoss:   {train_logloss:.4f}\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 2] Running 5-fold time series cross-validation...\n",
      "\n",
      "Fold 1: Acc=0.5281, AUC=0.5407, LogLoss=0.6906\n",
      "Fold 2: Acc=0.5553, AUC=0.5791, LogLoss=0.6831\n",
      "Fold 3: Acc=0.5412, AUC=0.5599, LogLoss=0.6864\n",
      "Fold 4: Acc=0.5462, AUC=0.5666, LogLoss=0.6857\n",
      "Fold 5: Acc=0.5507, AUC=0.5747, LogLoss=0.6839\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CROSS-VALIDATION PERFORMANCE\n",
      "--------------------------------------------------------------------------------\n",
      "Mean Accuracy: 0.5443 ± 0.0104\n",
      "Mean AUC:      0.5642 ± 0.0151\n",
      "Mean LogLoss:  0.6859 ± 0.0029\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[Step 2] Running 5-fold time series cross-validation...\\n\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_best), 1):\n",
    "    X_tr, X_val = X_train_best.iloc[train_idx], X_train_best.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_fold = xgb.XGBClassifier(**random_search.best_params_, random_state=42)\n",
    "    xgb_fold.fit(X_tr, y_tr, verbose=False)\n",
    "    xgb_val_probs = xgb_fold.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_fold = lgb.LGBMClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.7, random_state=42, verbose=-1\n",
    "    )\n",
    "    lgb_fold.fit(X_tr, y_tr)\n",
    "    lgb_val_probs = lgb_fold.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # CatBoost\n",
    "    cat_fold = CatBoostClassifier(\n",
    "        iterations=300, depth=6, learning_rate=0.05,\n",
    "        random_state=42, verbose=0\n",
    "    )\n",
    "    cat_fold.fit(X_tr, y_tr)\n",
    "    cat_val_probs = cat_fold.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Ensemble\n",
    "    ensemble_val_probs = (xgb_val_probs + lgb_val_probs + cat_val_probs) / 3\n",
    "    ensemble_val_preds = (ensemble_val_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_val, ensemble_val_preds)\n",
    "    auc = roc_auc_score(y_val, ensemble_val_probs)\n",
    "    logloss = log_loss(y_val, ensemble_val_probs)\n",
    "    \n",
    "    cv_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': acc,\n",
    "        'auc': auc,\n",
    "        'logloss': logloss\n",
    "    })\n",
    "    \n",
    "    print(f\"Fold {fold}: Acc={acc:.4f}, AUC={auc:.4f}, LogLoss={logloss:.4f}\")\n",
    "\n",
    "# CV Summary\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "cv_acc_mean = cv_df['accuracy'].mean()\n",
    "cv_auc_mean = cv_df['auc'].mean()\n",
    "cv_logloss_mean = cv_df['logloss'].mean()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CROSS-VALIDATION PERFORMANCE\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Mean Accuracy: {cv_acc_mean:.4f} ± {cv_df['accuracy'].std():.4f}\")\n",
    "print(f\"Mean AUC:      {cv_auc_mean:.4f} ± {cv_df['auc'].std():.4f}\")\n",
    "print(f\"Mean LogLoss:  {cv_logloss_mean:.4f} ± {cv_df['logloss'].std():.4f}\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERFITTING ANALYSIS\n",
      "================================================================================\n",
      "Train Accuracy:  0.6634\n",
      "CV Accuracy:     0.5443\n",
      "Difference:      0.1191 (+11.91pp)\n",
      "\n",
      "⚠️  WARNING: Significant overfitting detected (>3pp gap)\n",
      "   Consider: simpler model, fewer features, more regularization\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train Accuracy:  {train_acc:.4f}\")\n",
    "print(f\"CV Accuracy:     {cv_acc_mean:.4f}\")\n",
    "print(f\"Difference:      {(train_acc - cv_acc_mean):.4f} ({(train_acc - cv_acc_mean)*100:+.2f}pp)\")\n",
    "\n",
    "if train_acc - cv_acc_mean > 0.03:\n",
    "    print(\"\\n⚠️  WARNING: Significant overfitting detected (>3pp gap)\")\n",
    "    print(\"   Consider: simpler model, fewer features, more regularization\")\n",
    "elif train_acc - cv_acc_mean > 0.01:\n",
    "    print(\"\\n⚠️  Mild overfitting detected (1-3pp gap)\")\n",
    "    print(\"   Model might underperform on test set\")\n",
    "else:\n",
    "    print(\"\\n✓ Good generalization (gap <1pp)\")\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step 3] Making predictions on test set...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_probs = xgb_model.predict_proba(X_test_best)[:, 1]\n",
    "lgb_probs = lgb_model.predict_proba(X_test_best)[:, 1]\n",
    "cat_probs = cat_model.predict_proba(X_test_best)[:, 1]\n",
    "\n",
    "ensemble_probs = (xgb_probs + lgb_probs + cat_probs) / 3\n",
    "ensemble_preds = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_eng['ID'],\n",
    "    'RET': ensemble_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant overfitting, with submission score of 0.5006650544135429"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
